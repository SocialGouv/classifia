# ==================================
# Albert LLM Configuration
# ==================================
# API key for Albert LLM service (French government AI)
ALBERT_API_KEY=sk-your_albert_api_key_here
# Albert API base URL
ALBERT_URL=https://albert.api.etalab.gouv.fr/v1

# ==================================
# Database Configuration
# ==================================
# PostgreSQL connection string with pgvector extension
# Format: postgresql://user:password@host:port/database
DATABASE_URL=postgresql://postgres:postgres@localhost:5999/classifia

# ==================================
# Redis Configuration
# ==================================
# Redis URL
REDIS_URL=redis://127.0.0.1:6999

# ==================================
# Application Configuration
# ==================================
# Environment: development, test, or production
NODE_ENV=development
# Port for NestJS application
PORT=3000

# ==================================
# OpenAI Configuration
# ==================================
# Disable OpenAI Agents tracing (set to 1 to disable)
OPENAI_AGENTS_DISABLE_TRACING=1

# ==================================
# Crisp API Configuration
# ==================================
# Webhook secret for Crisp webhook authentication
CRISP_WEBHOOK_SECRET=your_crisp_webhook_secret_here
# Base64 encoded Crisp API key (format: identifier:key)
CRISP_API_KEY=your_base64_encoded_crisp_api_key_here
# Crisp API base URL with your website ID
CRISP_URL=https://api.crisp.chat/v1/website/your_website_id_here

# ==================================
# BullMQ Configuration
# ==================================
# Number of concurrent job processors
BULLMQ_CONCURRENCY=5
# Maximum number of retry attempts for failed jobs
BULLMQ_ATTEMPTS=3
# Delay in milliseconds before retrying a failed job
BULLMQ_BACKOFF_DELAY=30000
# Maximum number of jobs processed per minute (rate limiting)
BULLMQ_RATE_LIMIT=60

# ==================================
# Token & Processing Limits
# ==================================
# Maximum tokens per conversation for LLM processing
MAX_TOKENS_PER_CONVERSATION=2000
# Maximum number of labels/subjects per conversation
MAX_LABELS_PER_CONVERSATION=5

# ==================================
# Hierarchical Classification Configuration
# ==================================
# Minimum similarity score to reuse existing label (0.0-1.0)
# Very high threshold to avoid duplicate labels
LABEL_SIMILARITY_THRESHOLD=0.92

# Minimum similarity score for topic retrieval via RAG (0.0-1.0)
# Medium threshold for conceptual similarity
TOPIC_SIMILARITY_THRESHOLD=0.75

# Maximum number of topics a label can be assigned to
# Allows multi-topic classification (e.g., email problem + document deposit)
TOPIC_ASSIGNMENT_MAX_TOPICS=3

# Number of similar topics to retrieve during RAG search
# Higher = more context for LLM decision, but slower
RAG_RETRIEVE_TOPICS_LIMIT=5